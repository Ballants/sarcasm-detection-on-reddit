{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_XjNG35q-_c"
   },
   "source": [
    "# **Sarcasm detection on Reddit**\n",
    "\n",
    "In this notebook, we addressed a binary classification problem aimed at detecting sarcasm in Reddit comments.\n",
    "\n",
    "We chose 3 feature engineering methods and 4 classification models and compared them to find out which combination performs better:\n",
    "- Feature engineering: TF-IDF, Word2Vec, and BERT.\n",
    "\n",
    "- Classification models: Logistic regression, Random forest, Linear SVC, and Multilayer perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MxG9URvZX6F"
   },
   "source": [
    "## **Spark Setup**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3TLMiGeRdiI"
   },
   "source": [
    "### 1. Install PySpark and related dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_m0ZZHdpAL1",
    "outputId": "418988dd-a9dc-42aa-928a-a12522a2ddf4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# java\n",
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# !wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop2.7.tgz\n",
    "# !tar xf spark-3.2.4-bin-hadoop2.7.tgz\n",
    "# !rm -rf /content/spark-3.2.4-bin-hadoop2.7.tgz\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yTWh4NkcrFqF",
    "outputId": "18daabc3-17c5-4d87-8183-13fd9c0503ea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Krt1RPb1rK53",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setting java path as environment variable\n",
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GX32q-n1rLbb",
    "outputId": "db8dbc7e-f7fe-424b-944f-f0c1c802b4a8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "!pip install spark-nlp==4.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZT5w4kaYGcZ",
    "outputId": "7c60b2f6-677e-4fb8-edfa-e38934486a14",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kw7DLnymt5lD",
    "outputId": "916bf10f-00c2-4729-94dc-f933407ff29e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAJfhtqAgHep"
   },
   "source": [
    "### 2. Import useful Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNwfUBoUgNLP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.feature import VectorAssembler, SQLTransformer, Normalizer\n",
    "from pyspark.sql.functions import udf, col, transform\n",
    "\n",
    "import sparknlp\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7SUuHFUgfhs"
   },
   "source": [
    "### 3. Create Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ak2WG_p3uaIi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Hj0MDQTghZ0",
    "outputId": "ccb0a44c-c14e-436f-d437-b2b3b13c36ce",
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"1g\") \\\n",
    "    .config(\"spark.driver.memory\",\"32G\") \\\n",
    "    .config(\"spark.python.worker.memory\",\"32G\") \\\n",
    "    .config(\"spark.sql.analyzer.maxIterations\", \"6000\") \\\n",
    "    .config(\"spark.driver.cores\", \"10\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"32G\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.4.2\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "print(\"Apache Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xi0BzByXufxI",
    "outputId": "712db94c-d174-4350-ae3f-cd2855d0669a"
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "type(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvgLAqZqvG5L"
   },
   "source": [
    "## **Dataset**\n",
    "\n",
    "The original dataset used in this notebook is available [here](https://www.kaggle.com/datasets/toygarr/datasets-for-natural-language-processing), in the *sarcasm* folder.\n",
    "This dataset contains 30k sarcastic comments from the Internet commentary website Reddit.\n",
    "The comments are pre-processed by making lower case and removing punctuations, hashtags, usernames and html tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSWbPHmjuCBS"
   },
   "source": [
    "### Download dataset from Kaggle\n",
    "\n",
    "In order to get the dataset from Kaggle library, a kaggle.json file is required. It can be downloaded from our personal account page and uploaded in the same folder of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9FqfesOeZ2li",
    "outputId": "fce0a571-007b-4a62-a0fa-03f12d09f0a0"
   },
   "outputs": [],
   "source": [
    "# 1. Install the Kaggle library\n",
    "! pip install kaggle\n",
    "\n",
    "# 2. Make a directory named “.kaggle”\n",
    "! mkdir ~/.kaggle\n",
    "\n",
    "# 3. Copy the “kaggle.json” into this new directory\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "\n",
    "# 4. Allocate the required permission for this file.\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# 5. Download the dataset\n",
    "! kaggle datasets download toygarr/datasets-for-natural-language-processing\n",
    "\n",
    "# unzip the directory\n",
    "! unzip /content/datasets-for-natural-language-processing.zip\n",
    "\n",
    "# delete zip file\n",
    "! rm -rf /content/datasets-for-natural-language-processing.zip\n",
    "\n",
    "# Make a directory to save final models\n",
    "! mkdir models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfCVZgoavfcb"
   },
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xl1HNtGU0e5Y",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "\n",
    "    df1 = spark.read.csv(\"data/sarcasm/train.csv\", header=True)\n",
    "    df2 = spark.read.csv(\"data/sarcasm/test.csv\", header=True)\n",
    "\n",
    "    df = df1.union(df2)\n",
    "\n",
    "    #####################################\n",
    "\n",
    "    # rename columns\n",
    "    print(\"Dataframe size (n. of rows): {:d}\\n\".format(df.count()))\n",
    "    df = df.withColumnRenamed(\"Y\", \"label\")\n",
    "    df = df.withColumnRenamed(\"text\", \"comment\")\n",
    "    print(\"Dataframe schema:\")\n",
    "    df.printSchema()\n",
    "    df.show(10)\n",
    "\n",
    "    #####################################\n",
    "\n",
    "    print(\"Number of NULL comments: {:d}\".format(df.where(col(\"comment\").isNull()).count()))\n",
    "    print(\"Number of NULL labels: {:d}\\n\".format(df.where(col(\"label\").isNull()).count()))\n",
    "    # remove NULL entry/ies\n",
    "    df = df.na.drop(subset=[\"comment\"])\n",
    "    # remove NULL labels\n",
    "    df = df.na.drop(subset=[\"label\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PM9hJLdTeceE"
   },
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kw3sn1DBkdbW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf, col, rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIr7_usce3z-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(df, column_name=\"comment\"):\n",
    "\n",
    "    # Clean text\n",
    "    print(\"***** Clean text *****\\n\")\n",
    "\n",
    "    cleaned_df = df.dropDuplicates([\"comment\", \"label\"])\n",
    "\n",
    "    # remove rows with same comments but different labels\n",
    "    cleaned_df_2 = cleaned_df.groupby(\"comment\").agg(count(\"comment\").alias(\"polletti\")).filter(col(\"polletti\")>1)\n",
    "    cleaned_df = cleaned_df.join(cleaned_df_2, [\"comment\"], 'left_anti')\n",
    "\n",
    "    cleaned_df_2.unpersist()\n",
    "    del cleaned_df_2\n",
    "\n",
    "    cleaned_df.show(10)\n",
    "\n",
    "    print(\"Let's see our dataset's class distribution:\")\n",
    "    cleaned_df.groupBy('label').count().show()\n",
    "\n",
    "    # The dataset is not balanced, we apply a downsampling so that the two classes had the same number of samples.\n",
    "    df_0 = cleaned_df.filter(col(\"label\") == 0).orderBy(rand()).limit(cleaned_df.filter(col(\"label\") == 1).count())\n",
    "    df_1 = cleaned_df.filter(col(\"label\") == 1)\n",
    "\n",
    "    cleaned_df = df_0.union(df_1)\n",
    "\n",
    "    print(\"Let's check if our dataset is now balanced:\")\n",
    "    # Now we have 13_552 samples per class\n",
    "    cleaned_df.groupBy('label').count().show()\n",
    "\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5A1iplQHqZt"
   },
   "source": [
    "## **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgeXosep6Ibf"
   },
   "source": [
    "### **BERT Sentence**\n",
    "\n",
    "*Bidirectional Encoder Representations from Transformers* (BERT) is a state-of-the-art transformer-based model for natural language processing (NLP). It is designed to capture contextual information from text by using a deep bidirectional architecture.\n",
    "BERT generates contextualized word embeddings, known as BERT embeddings, which are pre-trained on large amounts of unlabeled text and then fine-tuned for specific NLP tasks.\n",
    "\n",
    "The BERT model used in this notebook provides a **sentence-level embedding** using the BERT architecture. It takes as input a sequence of sentences and outputs a single embedding vector representing the entire sentence. The model captures the contextual information of the entire sentence and produces a fixed-length representation that captures the overall meaning or sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNQqJBGEqovN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import BertSentenceEmbeddings\n",
    "from sparknlp import EmbeddingsFinisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pKj3IJD0e5a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bert_sentence_embedding(df, save_parquet=False):\n",
    "\n",
    "    # convert the text into a Spark-NLP annotator-ready form\n",
    "    documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('comment') \\\n",
    "     .setOutputCol('document')\n",
    "\n",
    "    # sent_bert_base_uncased -> the model is imported from https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\n",
    "    embeddings = BertSentenceEmbeddings.pretrained(\"sent_bert_base_uncased\", \"en\") \\\n",
    "                .setInputCols(\"document\") \\\n",
    "                .setOutputCol(\"embeddings\")\n",
    "\n",
    "    finisher = EmbeddingsFinisher() \\\n",
    "              .setInputCols(\"embeddings\") \\\n",
    "              .setOutputCols(\"embeddings_result\")\n",
    "\n",
    "    bert_pipeline = Pipeline(stages=[documentAssembler, embeddings, finisher])\n",
    "\n",
    "    df_bert = bert_pipeline.fit(df).transform(df) \\\n",
    "                         .select(col('comment'), col('label').cast(FloatType()), explode(\"embeddings_result\").alias('features')) \\\n",
    "                         .select(['comment'] + ['label'] + [expr('features[' + str(x) + ']') for x in range(768)])\n",
    "\n",
    "    ### VectorAssembler ###\n",
    "    vector_assembler = VectorAssembler(inputCols=df_bert.columns[2:], outputCol='features').setHandleInvalid(\"skip\")\n",
    "\n",
    "    features_assembled = vector_assembler.transform(df_bert).select('comment', 'features', 'label')\n",
    "\n",
    "    df_bert.unpersist()\n",
    "    del df_bert\n",
    "\n",
    "    print(\"Dataframe schema:\")\n",
    "    features_assembled.printSchema()\n",
    "\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    if save_parquet:\n",
    "        path_parquet = \"bert_sentence_features.parquet\"\n",
    "        features_assembled.write.mode(\"overwrite\").parquet(path_parquet)\n",
    "\n",
    "    return features_assembled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ema1oA3zqgHF"
   },
   "source": [
    "### Split dataset\n",
    "Split original dataset into 2 subdatasets: training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FmaKWHeqqhdi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_split(df_features):\n",
    "    print(\"Dataframe size in data_split: {:d} instances\".format(df_features.count()))\n",
    "\n",
    "    # Randomly split our original dataset into 80÷20 for training and test, respectively\n",
    "    train_df, test_df = df_features.randomSplit([0.8, 0.2], seed = 42)\n",
    "\n",
    "    df_features.unpersist()\n",
    "    del df_features\n",
    "\n",
    "    print(\"Training set size: {:d} instances\".format(train_df.count()))\n",
    "    print(\"Test set size: {:d} instances\".format(test_df.count()))\n",
    "\n",
    "    print(\"\\nLet's verify our datasets are still balanced:\")\n",
    "    train_df.groupBy('label').count().show()\n",
    "    test_df.groupBy('label').count().show()\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF5vgp3FwhWZ"
   },
   "source": [
    "## **Classification models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZSKaQYZwnyB"
   },
   "source": [
    "### **Logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvELNZT6xNnK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUb40P1j9U_i",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logistic_regression_classifier(train_df, test_df, suffix_path=\"\", save_model=False):\n",
    "\n",
    "    lr = LogisticRegression(featuresCol='features',\n",
    "                          labelCol='label',\n",
    "                          maxIter=100,\n",
    "                          regParam=0.3,\n",
    "                          elasticNetParam=0.8)\n",
    "\n",
    "    ### Search for the best model's parameters. ###\n",
    "\n",
    "    # We use a ParamGridBuilder to construct a grid of parameters to search over\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.0, 0.05, 0.1]) \\\n",
    "                .addGrid(lr.elasticNetParam, [0.5, 0.8, 1.0]) \\\n",
    "                .build()\n",
    "\n",
    "    cross_val = CrossValidator(estimator=lr,\n",
    "                             estimatorParamMaps=param_grid,\n",
    "                             evaluator=BinaryClassificationEvaluator(metricName=\"areaUnderROC\"),\n",
    "                             numFolds=5,\n",
    "                             parallelism=5)\n",
    "\n",
    "    # Run cross-validation, and choose the best set of parameters.\n",
    "    cv_model = cross_val.fit(train_df)\n",
    "\n",
    "    best_lr = cv_model.bestModel\n",
    "\n",
    "    print(\"--- Best model's parameters: ---\\n- reParam = {:f}\\n- elasticNetParam = {:f}\".format(best_lr.getRegParam(), best_lr.getElasticNetParam()))\n",
    "\n",
    "    lr_predictions = best_lr.transform(test_df)\n",
    "    lr_prediction.show(10)\n",
    "\n",
    "    if save_model:\n",
    "        path = \"models/logisticRegression_\" + suffix_path\n",
    "        best_lr.write().overwrite().save(path)\n",
    "\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    return lr_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcQEUTaFmYBV"
   },
   "source": [
    "### **Random forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDWYCPMsmhHj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dIqrNFXiiJR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_forest_classifier(train_df, test_df, suffix_path='', save_model=False):\n",
    "\n",
    "    rf = RandomForestClassifier(featuresCol='features',\n",
    "                              labelCol='label')\n",
    "\n",
    "    ### Search for the best model's parameters. ###\n",
    "\n",
    "    # We use a ParamGridBuilder to construct a grid of parameters to search over\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [3, 5, 8]) \\\n",
    "    .addGrid(rf.numTrees, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "    cross_val = CrossValidator(estimator=rf,\n",
    "                             estimatorParamMaps=param_grid,\n",
    "                             evaluator=BinaryClassificationEvaluator(metricName=\"areaUnderROC\"),\n",
    "                             numFolds=5,\n",
    "                             parallelism=5)\n",
    "\n",
    "    # Run cross-validation, and choose the best set of parameters.\n",
    "    cv_model = cross_val.fit(train_df)\n",
    "\n",
    "    best_rf = cv_model.bestModel\n",
    "\n",
    "    print(\"--- Best model's parameters: ---\\n- maxDepth = {:d}\\n- numTrees = {:d}\".format(best_rf.getMaxDepth(), best_rf.getNumTrees()))\n",
    "\n",
    "    rf_predictions = best_rf.transform(test_df)\n",
    "    rf_predictions.show(10)\n",
    "\n",
    "    if save_model:\n",
    "        path = \"models/randomForest_\" + suffix_path\n",
    "        best_rf.write().overwrite().save(path)\n",
    "\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    return rf_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyj7FtWimhir",
    "tags": []
   },
   "source": [
    "### **Linear Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjmtSKKXmlw_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ic7Yzu_BdsH7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_svc_classifier(train_df, test_df, suffix_path='', save_model=False):\n",
    "\n",
    "    lsvc = LinearSVC(featuresCol='features', labelCol='label',\n",
    "                   maxIter=10, regParam=0.1)\n",
    "\n",
    "    ### Search for the best model's parameters. ###\n",
    "    \n",
    "    # We use a ParamGridBuilder to construct a grid of parameters to search over\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "                .addGrid(lsvc.regParam, [0.0, 0.05, 0.1]) \\\n",
    "                .addGrid(lsvc.maxIter, [50, 100]) \\\n",
    "                .build()\n",
    "\n",
    "    cross_val = CrossValidator(estimator=lsvc,\n",
    "                             estimatorParamMaps=param_grid,\n",
    "                             evaluator=BinaryClassificationEvaluator(metricName=\"areaUnderROC\"),\n",
    "                             numFolds=5,\n",
    "                             parallelism=5)\n",
    "\n",
    "    # Run cross-validation, and choose the best set of parameters.\n",
    "    cv_model = cross_val.fit(train_df)\n",
    "\n",
    "    best_lsvc = cv_model.bestModel\n",
    "\n",
    "    print(\"--- Best model's parameters: ---\\n- regParam = {:f}\\n- maxIter = {:d}\".format(best_lsvc.getRegParam(), best_lsvc.getMaxIter()))\n",
    "\n",
    "    lsvc_predictions = best_lsvc.transform(test_df)\n",
    "    lsvc_predictions.show(10)\n",
    "\n",
    "    if save_model:\n",
    "        path = \"models/linearSVC_\" + suffix_path\n",
    "        best_lsvc.write().overwrite().save(path)\n",
    "\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    return lsvc_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uf8ZO9nTmlPQ"
   },
   "source": [
    "### **Multilayer perceptron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaNVvY-8mqqp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-qlmPpDLa0H",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mlp_classifier(train_df, test_df, suffix_path='', save_model=False):\n",
    "\n",
    "    # specify layers for the neural network:\n",
    "    input_layer = 768 # features\n",
    "    output_layer = 2 # number of classes\n",
    "    layers = [input_layer, 64, 32, output_layer]\n",
    "\n",
    "    mlp = MultilayerPerceptronClassifier(featuresCol='features', labelCol='label',\n",
    "                                       maxIter=100, layers=layers, blockSize=256, seed=42)\n",
    "\n",
    "    mlp_model = mlp.fit(train_df)\n",
    "\n",
    "    print(\"--- Model's parameters: ---\\n- layers = {:s}\".format(str(mlp_model.getLayers())))\n",
    "\n",
    "    mlp_predictions = mlp_model.transform(test_df)\n",
    "    mlp_predictions.show(10)\n",
    "\n",
    "    if save_model:\n",
    "        path = \"models/mlp_\" + suffix_path\n",
    "        mlp_model.write().overwrite().save(path)\n",
    "\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    return mlp_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CN2C01Gx7Y14"
   },
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "178qa1FbMvcg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, matthews_corrcoef, roc_auc_score, average_precision_score\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuDinRMUp_QK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluation(model_predictions):\n",
    "    print(\"***** Test Set *****\")\n",
    "\n",
    "    preds_df = model_predictions.select(\"comment\", \"label\", \"prediction\").toPandas()\n",
    "\n",
    "    print(\"\\nShow some examples of miss-predictions: \")\n",
    "    display(preds_df[preds_df['prediction'] != preds_df['label']].head(100))\n",
    "\n",
    "    print(\"\\nClassification report:\")\n",
    "    clf_report = classification_report(y_true=preds_df['label'], y_pred=preds_df['prediction'], zero_division=0)\n",
    "    print(clf_report)\n",
    "\n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "    evaluator.setRawPredictionCol('rawPrediction')\n",
    "\n",
    "    # calculate AUROC\n",
    "    print('Area Under ROC Curve (AUROC): %.3f' % evaluator.evaluate(model_predictions, {evaluator.metricName: \"areaUnderROC\"}))\n",
    "\n",
    "    # calculate AUPR\n",
    "    print('Area Under Precision-Recall Curve (AUPR): %.3f' % evaluator.evaluate(model_predictions, {evaluator.metricName: \"areaUnderPR\"}))\n",
    "\n",
    "    # calculate MCC\n",
    "    print(\"Matthews Correlation Coefficient (MCC): \", matthews_corrcoef(preds_df['label'], preds_df['prediction']))\n",
    "\n",
    "    print(\"\\n***** Test Set *****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noKx6G_Gp-5S"
   },
   "source": [
    "## **Experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOevFnCIWndJ"
   },
   "source": [
    "### **Data loading and pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3DxkBnx_KQj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-HIQPRt_Ne8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_df = clean_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXv09wBm_fDU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.unpersist()\n",
    "del df\n",
    "\n",
    "print(\"Garbage collector: collected %d objects\" % (gc.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raEKRjhk0e5i",
    "tags": []
   },
   "source": [
    "### **Models + BERT Sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzyO0PF6CcUq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features_df_bert = spark.read.parquet(\"bert_sentence_features.parquet\")\n",
    "features_df_bert = bert_sentence_embedding(cleaned_df, save_parquet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iN4mSf6Q0e5i",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_df.unpersist()\n",
    "del cleaned_df\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfuQ8_TIvM8O",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_bert, test_df_bert = data_split(features_df_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cr80lE2_vNUe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_df_bert.unpersist()\n",
    "del features_df_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cb5S9viHFhly",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = logistic_regression_classifier(train_df_bert, test_df_bert, \"BERT_sentence\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOhAs55Tkdcm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = random_forest_classifier(train_df_bert, test_df_bert, \"BERT_sentence\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2A6lv1zFVsfq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = linear_svc_classifier(train_df_bert, test_df_bert, \"BERT_sentence\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UnCvG3iVsyZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = mlp_classifier(train_df_bert, test_df_bert, \"BERT_sentence\",save_model=True)\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UiYFeLWw0e5k"
   },
   "outputs": [],
   "source": [
    "train_df_bert.unpersist()\n",
    "test_df_bert.unpersist()\n",
    "\n",
    "del train_df_bert\n",
    "del test_df_bert"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

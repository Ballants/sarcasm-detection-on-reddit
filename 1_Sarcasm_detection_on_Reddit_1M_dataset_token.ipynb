{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_XjNG35q-_c"
   },
   "source": [
    "# **Sarcasm detection on Reddit**\n",
    "\n",
    "In this notebook, we addressed a binary classification problem aimed at detecting sarcasm in Reddit comments.\n",
    "\n",
    "We chose 3 feature engineering methods and 4 classification models and compared them to find out which combination performs better:\n",
    "- Feature engineering: TF-IDF, Word2Vec, and BERT.\n",
    "\n",
    "- Classification models: Logistic regression, Random forest, Linear SVC, and Multilayer perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MxG9URvZX6F"
   },
   "source": [
    "## **Spark Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSczgSH_FAIy"
   },
   "source": [
    "### 1. Install PySpark and related dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_m0ZZHdpAL1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# java\n",
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# !wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop2.7.tgz\n",
    "# !tar xf spark-3.2.4-bin-hadoop2.7.tgz\n",
    "# !rm -rf /content/spark-3.2.4-bin-hadoop2.7.tgz\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTWh4NkcrFqF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Krt1RPb1rK53",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setting java path as environment variable\n",
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GX32q-n1rLbb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "!pip install spark-nlp==4.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZT5w4kaYGcZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kw7DLnymt5lD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAJfhtqAgHep"
   },
   "source": [
    "### 2. Import useful Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNwfUBoUgNLP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.feature import VectorAssembler, SQLTransformer, Normalizer\n",
    "from pyspark.sql.functions import udf, col, lower, trim, regexp_replace, transform\n",
    "\n",
    "import sparknlp\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7SUuHFUgfhs"
   },
   "source": [
    "### 3. Create Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ak2WG_p3uaIi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Hj0MDQTghZ0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"1g\") \\\n",
    "    .config(\"spark.driver.memory\",\"32G\") \\\n",
    "    .config(\"spark.python.worker.memory\",\"32G\") \\\n",
    "    .config(\"spark.sql.analyzer.maxIterations\", \"6000\") \\\n",
    "    .config(\"spark.driver.cores\", \"10\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"32G\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.4.2\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "print(\"Apache Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xi0BzByXufxI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "type(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvgLAqZqvG5L"
   },
   "source": [
    "## **Dataset**\n",
    "\n",
    "The original dataset used for this task is available [here](https://www.kaggle.com/datasets/danofer/sarcasm).\n",
    "This dataset contains 1.3 million Sarcastic comments from the Internet commentary website Reddit.\n",
    "The dataset was generated by scraping comments from Reddit containing the `\\s` (sarcasm) tag. This tag is often used by Redditors to indicate that their comment is in jest and not meant to be taken seriously, and is generally a reliable indicator of sarcastic comment content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSWbPHmjuCBS"
   },
   "source": [
    "### Download dataset from Kaggle\n",
    "\n",
    "In order to get the dataset from Kaggle library, a kaggle.json file is required. It can be downloaded from our personal account page and uploaded in the same folder of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FqfesOeZ2li",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the Kaggle library\n",
    "! pip install kaggle\n",
    "\n",
    "# Make a directory named “.kaggle”\n",
    "! mkdir kaggle\n",
    "\n",
    "# Copy the “kaggle.json” into this new directory\n",
    "! cp kaggle.json kaggle/\n",
    "\n",
    "# Allocate the required permission for this file.\n",
    "! chmod 600 kaggle/kaggle.json\n",
    "\n",
    "# Download the dataset\n",
    "! kaggle datasets download danofer/sarcasm -f train-balanced-sarcasm.csv\n",
    "\n",
    "# Unzip the directory\n",
    "! unzip train-balanced-sarcasm.csv.zip\n",
    "\n",
    "# Delete zip file\n",
    "! rm -rf train-balanced-sarcasm.csv.zip\n",
    "\n",
    "# Make a directory to save final models\n",
    "! mkdir models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfCVZgoavfcb"
   },
   "source": [
    "### Data loading\n",
    "\n",
    "Due to a limited amount of resources at our disposal, in order to use the *BERT Embedding* we were forced to work with only part of the original dataset. Therefore, we randomly selected only 20% of the comments in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxOGIHBgr2q7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataset_partitioned(dataset_file_path = \"train-balanced-sarcasm.csv\", partitioned = False):\n",
    "\n",
    "    df_from_csv = spark.read.csv(\"train-balanced-sarcasm.csv\", header=True)\n",
    "\n",
    "    print(\"Dataframe size (n. of rows): {:d}\\n\".format(df_from_csv.count()))\n",
    "    print(\"Dataframe schema:\")\n",
    "    df_from_csv.printSchema()\n",
    "    df_from_csv.show(10)\n",
    "\n",
    "    # selecting only the columns we are interested in\n",
    "    df = df_from_csv.drop(\"author\", \"subreddit\", \"score\", \"ups\", \"downs\", \"date\", \"created_utc\", \"parent_comment\")\n",
    "\n",
    "    if partitioned:\n",
    "        # selecting 20% of the dataset\n",
    "        df, second_partition = df.randomSplit([0.1, 0.9], seed = 42)\n",
    "        second_partition.unpersist()\n",
    "        del second_partition\n",
    "\n",
    "    print(\"\\nSelecting only the columns we are interested in\")\n",
    "    print(\"Dataframe with comment length size (n. of rows): {:d}\\n\".format(df.count()))\n",
    "    print(\"Dataframe schema:\")\n",
    "    df.printSchema()\n",
    "    df.show(10)\n",
    "\n",
    "    print(\"Number of NULL entry/ies: {:d}\".format(df.where(col(\"comment\").isNull()).count()))\n",
    "    # remove NULL entry/ies\n",
    "    df = df.na.drop(subset=[\"comment\"])\n",
    "\n",
    "    print(\"Number of NULL labels: {:d}\\n\".format(df.where(col(\"label\").isNull()).count()))\n",
    "    # remove NULL labels\n",
    "    df = df.na.drop(subset=[\"label\"])\n",
    "\n",
    "    print(\"Let's verify our dataset is still balanced:\")\n",
    "    df.groupBy('label').count().show()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PM9hJLdTeceE"
   },
   "source": [
    "### Data Pre-processing\n",
    "\n",
    "Starting from the raw text data, we apply a standard NLP preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kw3sn1DBkdbW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer, LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIr7_usce3z-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(df, column_name=\"comment\"):\n",
    "\n",
    "    # Text preprocessing pipeline\n",
    "    print(\"***** Text Preprocessing Pipeline *****\\n\")\n",
    "\n",
    "    # convert the text into a Spark-NLP annotator-ready form\n",
    "    documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol(column_name) \\\n",
    "     .setOutputCol('document')\n",
    "\n",
    "    tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['document']) \\\n",
    "     .setOutputCol('token')\n",
    "\n",
    "    normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setLowercase(True) \\\n",
    "    .setCleanupPatterns([\"\"\"[^\\w\\d\\s]\"\"\"]) # remove punctuations (keep alphanumeric chars)\n",
    "\n",
    "    # note that lemmatizer needs a dictionary. So we used the pre-trained model (note that it defaults to english)\n",
    "    lemmatizer = LemmatizerModel.pretrained(\"lemma_antbnc\") \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('lemma')\n",
    "\n",
    "    stopwords_cleaner = StopWordsCleaner.pretrained(\"stopwords_en\") \\\n",
    "     .setInputCols(['lemma']) \\\n",
    "     .setOutputCol('clean_lemma') \\\n",
    "     .setCaseSensitive(False)\n",
    "\n",
    "    # convert tokens back to human-readable form\n",
    "    finisher = Finisher() \\\n",
    "     .setInputCols(['clean_lemma']) \\\n",
    "     .setOutputCols(\"finisher_result\") \\\n",
    "     .setCleanAnnotations(False) \\\n",
    "     .setOutputAsArray(True)\n",
    "\n",
    "    pipeline = Pipeline() \\\n",
    "     .setStages([\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        normalizer,\n",
    "        lemmatizer,\n",
    "        stopwords_cleaner,\n",
    "        finisher\n",
    "     ])\n",
    "\n",
    "    cleaned_df = pipeline.fit(df).transform(df).select(['comment', 'document', 'clean_lemma', 'finisher_result', 'label'])\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    cleaned_df = cleaned_df.filter(\"finisher_result != array() and finisher_result != array('') and finisher_result != array(null)\")\n",
    "    cleaned_df = cleaned_df.dropDuplicates([\"finisher_result\",\"label\"])\n",
    "\n",
    "    # remove rows with same comments but different labels\n",
    "    cleaned_df_2 = cleaned_df.groupby(\"finisher_result\").agg(count(\"finisher_result\").alias(\"polletti\")).filter(col(\"polletti\")>1)\n",
    "    cleaned_df = cleaned_df.join(cleaned_df_2, [\"finisher_result\"], 'left_anti')\n",
    "\n",
    "    cleaned_df_2.unpersist()\n",
    "    del cleaned_df_2\n",
    "\n",
    "    print(\"Dataframe schema:\")\n",
    "    cleaned_df.printSchema()\n",
    "\n",
    "    cleaned_df.show(10)\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    print(\"Let's verify our dataset is still balanced:\")\n",
    "    cleaned_df.groupBy('label').count().show()\n",
    "\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5A1iplQHqZt"
   },
   "source": [
    "## **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uH9bbYFXJKx-"
   },
   "source": [
    "### **TF-IDF + PCA**\n",
    "\n",
    "*Term frequency-inverse document frequency* (TF-IDF) is a feature vectorization method widely used in text mining to reflect the importance of a term to a document in the corpus.\n",
    "The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.\n",
    "\n",
    "*Principal component analysis* (PCA) is a dimensionality reduction technique, which is a process of reducing the number of variables under consideration.\n",
    "More specifically, it is a statistical method to find a rotation such that the first coordinate has the largest variance possible, and each succeeding coordinate, in turn, has the largest variance possible. The columns of the rotation matrix are called principal components.\\\n",
    "We use it to project the vector of extracted features into a low-dimensional space, thus selecting only the most important ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeYKO5c-CNkN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, CountVectorizer, IDF\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRDEUhwAHpxh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tfidf_pca_features(cleaned_df, column_name=\"finisher_result\", save_parquet=False, load_pipeline=False):\n",
    "\n",
    "    if load_pipeline:\n",
    "        features_pca_fitted = PipelineModel.load(\"PCA_pipeline\")\n",
    "    else:\n",
    "\n",
    "        ### TF-IDF ###\n",
    "        hashingTF = HashingTF(inputCol=column_name, outputCol=\"tf_features\", numFeatures=10_000)\n",
    "        idf = IDF(inputCol=\"tf_features\", outputCol=\"idf_features\")\n",
    "\n",
    "        ### StandardScaler ###\n",
    "        scaler = StandardScaler(inputCol=\"idf_features\",\n",
    "                              outputCol=\"std_features\",\n",
    "                              withStd=True, withMean=True)\n",
    "\n",
    "        ### PCA ###\n",
    "        pca_model = PCA(k=500, inputCol=\"std_features\", outputCol=\"pca_features\")\n",
    "\n",
    "        tf_idf_pca_pipeline = Pipeline(stages=[hashingTF, idf, scaler, pca_model])\n",
    "        features_pca_fitted = tf_idf_pca_pipeline.fit(cleaned_df)\n",
    "\n",
    "        features_pca_fitted.write().overwrite().save(\"PCA_pipeline\")\n",
    "\n",
    "\n",
    "    features_pca = features_pca_fitted.transform(cleaned_df) \\\n",
    "                          .select(col(\"comment\"), col(\"pca_features\").alias(\"features\"), col(\"label\")) \\\n",
    "                          .withColumn(\"features\", vector_to_array(\"features\")) \\\n",
    "                          .select(['comment'] + ['label'] + [expr('features[' + str(x) + ']') for x in range(500)])\n",
    "\n",
    "    # cast each column to float\n",
    "    for col_name in features_pca.columns:\n",
    "        if col_name == \"comment\":\n",
    "            continue\n",
    "        features_pca = features_pca.withColumn(col_name, col(col_name).cast(FloatType()))\n",
    "\n",
    "    ### VectorAssembler ###\n",
    "    vector_assembler = VectorAssembler(inputCols=features_pca.columns[2:], outputCol='features').setHandleInvalid(\"skip\")\n",
    "    features_pca = vector_assembler.transform(features_pca).select('comment', 'features', 'label')\n",
    "\n",
    "    print(\"Dataframe schema:\")\n",
    "    features_pca.printSchema()\n",
    "    features_pca.show(10)\n",
    "\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    if save_parquet:\n",
    "        features_pca.write.mode(\"overwrite\").parquet(\"pca_features.parquet\")\n",
    "\n",
    "    return features_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZLyZPQHOplX"
   },
   "source": [
    "### **Word2Vec**\n",
    "\n",
    "Word2Vec is a technique that uses a (shallow) neural-network-based model to map each word to a vector of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Weede8WEP84e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import Word2VecModel, SentenceEmbeddings, Word2VecApproach\n",
    "from sparknlp import EmbeddingsFinisher\n",
    "from pyspark.ml.functions import vector_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4X7jFP0Fr2q-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def word2vec_embedding(cleaned_df, pre_trained = False, save_parquet=True):\n",
    "\n",
    "    df_word2vec = cleaned_df.select('document', 'clean_lemma', 'label', 'comment')\n",
    "\n",
    "    cleaned_df.unpersist()\n",
    "    del cleaned_df\n",
    "\n",
    "    if pre_trained:\n",
    "        embeddings = Word2VecModel.pretrained(\"word2vec_gigaword_wiki_300\", \"en\") \\\n",
    "            .setInputCols([\"clean_lemma\"]) \\\n",
    "            .setOutputCol(\"embeddings\")\n",
    "    else:\n",
    "        embeddings = Word2VecApproach() \\\n",
    "            .setInputCols([\"clean_lemma\"]) \\\n",
    "            .setOutputCol(\"embeddings\") \\\n",
    "            .setMaxIter(5) \\\n",
    "            .setVectorSize(300)\n",
    "\n",
    "    # the result of w2v are multiple vectors, we will collapse them into one using the average pooling strategy\n",
    "    embeddingsSentence = SentenceEmbeddings() \\\n",
    "      .setInputCols([\"document\", \"embeddings\"]) \\\n",
    "      .setOutputCol(\"sentence_embeddings\") \\\n",
    "      .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "    embeddingsFinisher = EmbeddingsFinisher() \\\n",
    "    .setInputCols(\"sentence_embeddings\") \\\n",
    "    .setOutputCols(\"embeddings_result\") \\\n",
    "    .setOutputAsVector(True) \\\n",
    "    .setCleanAnnotations(False)\n",
    "\n",
    "    word2vec_pipeline = Pipeline(stages=[embeddings, embeddingsSentence, embeddingsFinisher])\n",
    "\n",
    "    df_word2vec = word2vec_pipeline.fit(df_word2vec).transform(df_word2vec) \\\n",
    "                                 .select(col(\"comment\"), col('label'), explode(\"embeddings_result\").alias('features')) \\\n",
    "                                 .withColumn(\"features\", vector_to_array(\"features\")) \\\n",
    "                                 .select(['comment'] + ['label'] + [expr('features[' + str(x) + ']') for x in range(300)])\n",
    "\n",
    "    # cast each column to float\n",
    "    for col_name in df_word2vec.columns:\n",
    "        if col_name == \"comment\":\n",
    "            continue\n",
    "        df_word2vec = df_word2vec.withColumn(col_name, col(col_name).cast(FloatType()))\n",
    "\n",
    "    ### VectorAssembler ###\n",
    "    vector_assembler = VectorAssembler(inputCols=df_word2vec.columns[2:], outputCol='features').setHandleInvalid(\"skip\")\n",
    "\n",
    "    features_assembled = vector_assembler.transform(df_word2vec).select('comment', 'features', 'label')\n",
    "\n",
    "    df_word2vec.unpersist()\n",
    "    del df_word2vec\n",
    "\n",
    "    print(\"Dataframe schema:\")\n",
    "    features_assembled.printSchema()\n",
    "    features_assembled.show(10)\n",
    "\n",
    "    if save_parquet:\n",
    "        features_assembled.write.mode(\"overwrite\").parquet(\"word2vec_features.parquet\")\n",
    "\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    return features_assembled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgeXosep6Ibf"
   },
   "source": [
    "### **BERT**\n",
    "\n",
    "*Bidirectional Encoder Representations from Transformers* (BERT) is a state-of-the-art transformer-based model for natural language processing (NLP). It is designed to capture contextual information from text by using a deep bidirectional architecture.\n",
    "BERT generates contextualized word embeddings, known as BERT embeddings, which are pre-trained on large amounts of unlabeled text and then fine-tuned for specific NLP tasks.\n",
    "\n",
    "The BERT models used in this notebook provides a **word-level embedding** using the BERT architecture. They take as input a sequence of tokens and generate contextualized embeddings for each token in the sequence. The models capture the context of each token based on its surrounding tokens in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNQqJBGEqovN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import BertEmbeddings, SentenceEmbeddings\n",
    "from sparknlp import EmbeddingsFinisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZZURiidCfwS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bert_embedding(cleaned_df, save_parquet=False, model=\"small_bert_L2_768\"):\n",
    "    df_bert = cleaned_df.select('document', 'clean_lemma', 'label', 'comment')\n",
    "\n",
    "    cleaned_df.unpersist()\n",
    "    del cleaned_df\n",
    "\n",
    "    # bert_base_uncased -> the model is imported from https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\n",
    "    # small_bert_L2_768 -> the model is imported from https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1\n",
    "    embeddings = BertEmbeddings.pretrained(model, 'en') \\\n",
    "                .setInputCols(\"document\", \"clean_lemma\") \\\n",
    "                .setOutputCol(\"embeddings\")\n",
    "    \n",
    "    # the result of BERT are multiple vectors, we will collapse them into one using the average pooling strategy\n",
    "    embeddingsSentence = SentenceEmbeddings() \\\n",
    "      .setInputCols([\"document\", \"embeddings\"]) \\\n",
    "      .setOutputCol(\"sentence_embeddings\") \\\n",
    "      .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "    finisher = EmbeddingsFinisher() \\\n",
    "              .setInputCols(\"sentence_embeddings\") \\\n",
    "              .setOutputCols(\"embeddings_result\")\n",
    "\n",
    "    bert_pipeline = Pipeline(stages=[embeddings, embeddingsSentence, finisher])\n",
    "\n",
    "    df_bert = bert_pipeline.fit(df_bert).transform(df_bert) \\\n",
    "                         .select(col(\"comment\"), col('label').cast(FloatType()), explode(\"embeddings_result\").alias('features')) \\\n",
    "                         .select(['comment'] + ['label'] + [expr('features[' + str(x) + ']') for x in range(768)])\n",
    "\n",
    "    ### VectorAssembler ###\n",
    "    vector_assembler = VectorAssembler(inputCols=df_bert.columns[2:], outputCol='features').setHandleInvalid(\"skip\")\n",
    "\n",
    "    features_assembled = vector_assembler.transform(df_bert).select('comment', 'features', 'label')\n",
    "    \n",
    "    df_bert.unpersist()\n",
    "    del df_bert\n",
    "\n",
    "    print(\"Dataframe schema:\")\n",
    "    features_assembled.printSchema()\n",
    "\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    if save_parquet:\n",
    "        features_assembled.write.mode(\"overwrite\").parquet(\"bert_features_768.parquet\")\n",
    "\n",
    "    return features_assembled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aK_hlfXgywH8"
   },
   "source": [
    "### Split dataset\n",
    "Split original dataset into 2 subdatasets: training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWVFXzRpEIJz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_split(df_features):\n",
    "    print(\"Dataframe size in data_split: {:d} instances\".format(df_features.count()))\n",
    "\n",
    "    # Randomly split our original dataset into 80÷20 for training and test, respectively\n",
    "    train_df, test_df = df_features.randomSplit([0.8, 0.2], seed = 42)\n",
    "\n",
    "    df_features.unpersist()\n",
    "    del df_features\n",
    "\n",
    "    print(\"Training set size: {:d} instances\".format(train_df.count()))\n",
    "    print(\"Test set size: {:d} instances\".format(test_df.count()))\n",
    "\n",
    "    print(\"\\nLet's verify our datasets are still balanced:\")\n",
    "    train_df.groupBy('label').count().show()\n",
    "    test_df.groupBy('label').count().show()\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF5vgp3FwhWZ"
   },
   "source": [
    "## **Classification models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZSKaQYZwnyB"
   },
   "source": [
    "### **Logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvELNZT6xNnK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUb40P1j9U_i",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logistic_regression_classifier(train_df, test_df, suffix_path=\"\", save_model=False):\n",
    "\n",
    "    lr = LogisticRegression(featuresCol='features',\n",
    "                          labelCol='label',\n",
    "                          maxIter=100,\n",
    "                          regParam=0.3,\n",
    "                          elasticNetParam=0.8)\n",
    "\n",
    "    ### Search for the best model's parameters. ###\n",
    "    \n",
    "    # We use a ParamGridBuilder to construct a grid of parameters to search over\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.0, 0.05, 0.1]) \\\n",
    "                .addGrid(lr.elasticNetParam, [0.5, 0.8, 1.0]) \\\n",
    "                .build()\n",
    "\n",
    "    cross_val = CrossValidator(estimator=lr,\n",
    "                             estimatorParamMaps=param_grid,\n",
    "                             evaluator=BinaryClassificationEvaluator(metricName=\"areaUnderROC\"),\n",
    "                             numFolds=5,\n",
    "                             parallelism=5)\n",
    "\n",
    "    # Run cross-validation, and choose the best set of parameters.\n",
    "    cv_model = cross_val.fit(train_df)\n",
    "\n",
    "    best_lr = cv_model.bestModel\n",
    "\n",
    "    print(\"--- Best model's parameters: ---\\n- reParam = {:f}\\n- elasticNetParam = {:f}\".format(best_lr.getRegParam(), best_lr.getElasticNetParam()))\n",
    "\n",
    "    lr_predictions = best_lr.transform(test_df)\n",
    "    lr_prediction.show(10)\n",
    "\n",
    "    if save_model:\n",
    "        path = \"models/logisticRegression_\" + suffix_path\n",
    "        best_lr.write().overwrite().save(path)\n",
    "\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    return lr_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcQEUTaFmYBV"
   },
   "source": [
    "### **Random forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDWYCPMsmhHj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dIqrNFXiiJR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_forest_classifier(train_df, test_df, suffix_path='',save_model=False):\n",
    "\n",
    "    rf = RandomForestClassifier(featuresCol='features',\n",
    "                              labelCol='label')\n",
    "\n",
    "    ### Search for the best model's parameters. ###\n",
    "\n",
    "    # We use a ParamGridBuilder to construct a grid of parameters to search over\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [3, 5, 8]) \\\n",
    "    .addGrid(rf.numTrees, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "    cross_val = CrossValidator(estimator=rf,\n",
    "                             estimatorParamMaps=param_grid,\n",
    "                             evaluator=BinaryClassificationEvaluator(metricName=\"areaUnderROC\"),\n",
    "                             numFolds=5,\n",
    "                             parallelism=5)\n",
    "\n",
    "    # Run cross-validation, and choose the best set of parameters.\n",
    "    cv_model = cross_val.fit(train_df)\n",
    "\n",
    "    best_rf = cv_model.bestModel\n",
    "\n",
    "    print(\"--- Best model's parameters: ---\\n- maxDepth = {:d}\\n- numTrees = {:d}\".format(best_rf.getMaxDepth(), best_rf.getNumTrees()))\n",
    "\n",
    "    rf_predictions = best_rf.transform(test_df)\n",
    "    rf_predictions.show(10)\n",
    "\n",
    "    if save_model:\n",
    "        path = \"models/randomForest_\" + suffix_path\n",
    "        best_rf.write().overwrite().save(path)\n",
    "\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    return rf_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyj7FtWimhir",
    "tags": []
   },
   "source": [
    "### **Linear Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjmtSKKXmlw_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ic7Yzu_BdsH7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_svc_classifier(train_df, test_df, suffix_path='', save_model=False):\n",
    "\n",
    "    lsvc = LinearSVC(featuresCol='features', labelCol='label', regParam=0.1)\n",
    "\n",
    "    ### Search for the best model's parameters. ###\n",
    "\n",
    "    # We use a ParamGridBuilder to construct a grid of parameters to search over\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "                .addGrid(lsvc.regParam, [0.0, 0.05, 0.1]) \\\n",
    "                .addGrid(lsvc.maxIter, [50, 100]) \\\n",
    "                .build()\n",
    "\n",
    "    cross_val = CrossValidator(estimator=lsvc,\n",
    "                             estimatorParamMaps=param_grid,\n",
    "                             evaluator=BinaryClassificationEvaluator(metricName=\"areaUnderROC\"),\n",
    "                             numFolds=5,\n",
    "                             parallelism=5)\n",
    "\n",
    "    # Run cross-validation, and choose the best set of parameters.\n",
    "    cv_model = cross_val.fit(train_df)\n",
    "\n",
    "    best_lsvc = cv_model.bestModel\n",
    "\n",
    "    print(\"--- Best model's parameters: ---\\n- regParam = {:f}\\n- maxIter = {:f}\".format(best_lsvc.getRegParam(), best_lsvc.getMaxIter()))\n",
    "\n",
    "    lsvc_predictions = best_lsvc.transform(test_df)\n",
    "    lsvc_predictions.show(10)\n",
    "\n",
    "    if save_model:\n",
    "        path = \"models/linearSVC_\" + suffix_path\n",
    "        best_lsvc.write().overwrite().save(path)\n",
    "\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    return lsvc_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uf8ZO9nTmlPQ"
   },
   "source": [
    "### **Multilayer perceptron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaNVvY-8mqqp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-qlmPpDLa0H",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mlp_classifier(train_df, test_df, suffix_path='', save_model=False):\n",
    "\n",
    "    if suffix_path == 'Word2Vec':\n",
    "        num_features = 300\n",
    "    elif suffix_path == 'tfidf':\n",
    "        num_features = 500\n",
    "    elif suffix_path == 'BERT_uncased' or suffix_path == 'BERT_small':\n",
    "        num_features = 768\n",
    "    else:\n",
    "        num_features = 1\n",
    "\n",
    "    # specify layers for the neural network:\n",
    "    input_layer = num_features # features\n",
    "    output_layer = 2 # number of classes\n",
    "    layers = [input_layer, 64, 32, output_layer]\n",
    "\n",
    "    mlp = MultilayerPerceptronClassifier(featuresCol='features', labelCol='label',\n",
    "                                       maxIter=100, layers=layers, blockSize=256, seed=42)\n",
    "\n",
    "    # Run cross-validation, and choose the best set of parameters.\n",
    "    mlp_model = mlp.fit(train_df)\n",
    "\n",
    "    print(\"--- Model's parameters: ---\\n- layers = {:s}\".format(str(mlp_model.getLayers())))\n",
    "        \n",
    "    mlp_predictions = mlp_model.transform(test_df)\n",
    "    mlp_predictions.show(10)\n",
    "\n",
    "    if save_model:\n",
    "        path = \"models/mlp_\" + suffix_path\n",
    "        mlp_model.write().overwrite().save(path)\n",
    "\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    return mlp_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CN2C01Gx7Y14"
   },
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "178qa1FbMvcg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuDinRMUp_QK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluation(model_predictions):\n",
    "    print(\"***** Test Set *****\")\n",
    "\n",
    "    preds_df = model_predictions.select(\"comment\", \"label\", \"prediction\").toPandas()\n",
    "\n",
    "    print(\"\\nShow some examples of miss-predictions: \")\n",
    "    display(preds_df[preds_df['prediction'] != preds_df['label']].head(100))\n",
    "\n",
    "    print(\"\\nClassification report:\")\n",
    "    clf_report = classification_report(y_true=preds_df['label'], y_pred=preds_df['prediction'], zero_division=0)\n",
    "    print(clf_report)\n",
    "\n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "    evaluator.setRawPredictionCol('rawPrediction')\n",
    "\n",
    "    # calculate AUROC\n",
    "    print('Area Under ROC Curve (AUROC): %.3f' % evaluator.evaluate(model_predictions, {evaluator.metricName: \"areaUnderROC\"}))\n",
    "\n",
    "    # calculate AUPR\n",
    "    print('Area Under Precision-Recall Curve (AUPR): %.3f' % evaluator.evaluate(model_predictions, {evaluator.metricName: \"areaUnderPR\"}))\n",
    "\n",
    "    # calculate MCC\n",
    "    print(\"Matthews Correlation Coefficient (MCC): \", matthews_corrcoef(preds_df['label'], preds_df['prediction']))\n",
    "\n",
    "    print(\"\\n***** Test Set *****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noKx6G_Gp-5S"
   },
   "source": [
    "## **Experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qy8e0jDTE4rL"
   },
   "source": [
    "### Data loading and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3DxkBnx_KQj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = load_dataset_partitioned(partitioned=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-HIQPRt_Ne8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_df = clean_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXv09wBm_fDU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.unpersist()\n",
    "del df\n",
    "\n",
    "print(\"Garbage collector: collected %d objects\" % (gc.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIOS5plAr2rO",
    "tags": []
   },
   "source": [
    "### Models + TF-IDF + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYLfcRITB236"
   },
   "outputs": [],
   "source": [
    "# features_df_tfidf = spark.read.parquet(\"tfidf_features.parquet\")\n",
    "features_df_tfidf = tfidf_pca_features(cleaned_df, save_parquet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEQLM9MkGZO_"
   },
   "outputs": [],
   "source": [
    "cleaned_df.unpersist()\n",
    "del cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bHx7eR-r2rO"
   },
   "outputs": [],
   "source": [
    "train_df_tfidf, test_df_tfidf = data_split(features_df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLtTJdxZr2rP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_df_tfidf.unpersist()\n",
    "del features_df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wO1V1eRvr2rP"
   },
   "outputs": [],
   "source": [
    "model_predictions = logistic_regression_classifier(train_df_tfidf, test_df_tfidf, \"tfidf\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grxsiPJer2rP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = random_forest_classifier(train_df_tfidf, test_df_tfidf, \"tfidf\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbGMqyUXr2rP"
   },
   "outputs": [],
   "source": [
    "model_predictions = linear_svc_classifier(train_df_tfidf, test_df_tfidf, \"tfidf\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXhUJ27yr2rQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = mlp_classifier(train_df_tfidf, test_df_tfidf, \"tfidf\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yNPm0hfr2rQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_tfidf.unpersist()\n",
    "test_df_tfidf.unpersist()\n",
    "\n",
    "del train_df_tfidf\n",
    "del test_df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFe7_6KEr2rK",
    "tags": []
   },
   "source": [
    "### Models + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2wq3b9O_h5U"
   },
   "outputs": [],
   "source": [
    "# features_df_word2vec = spark.read.parquet(\"word2vec_features.parquet\")\n",
    "features_df_word2vec = word2vec_embedding(cleaned_df, save_parquet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRchH6L7GnSW"
   },
   "outputs": [],
   "source": [
    "cleaned_df.unpersist()\n",
    "del cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUn5X_hUFaiC"
   },
   "outputs": [],
   "source": [
    "train_df_word2Vec, test_df_word2Vec = data_split(features_df_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGpiPrWXr2rM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_df_word2vec.unpersist()\n",
    "del features_df_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cb5S9viHFhly"
   },
   "outputs": [],
   "source": [
    "model_predictions = logistic_regression_classifier(train_df_word2Vec, test_df_word2Vec, \"Word2Vec\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOhAs55Tkdcm"
   },
   "outputs": [],
   "source": [
    "model_predictions = random_forest_classifier(train_df_word2Vec, test_df_word2Vec, \"Word2Vec\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UnCvG3iVsyZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = mlp_classifier(train_df_word2Vec, test_df_word2Vec, \"Word2Vec\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2A6lv1zFVsfq"
   },
   "outputs": [],
   "source": [
    "model_predictions = linear_svc_classifier(train_df_word2Vec, test_df_word2Vec, \"Word2Vec\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIh6We-vr2rO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_word2Vec.unpersist()\n",
    "test_df_word2vec.unpersist()\n",
    "\n",
    "del train_df_word2Vec\n",
    "del test_df_word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKHu_xJLFgeK"
   },
   "source": [
    "### Models + Word2Vec (pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUA6rC4-Fc6j"
   },
   "outputs": [],
   "source": [
    "features_df_word2vec = word2vec_embedding(cleaned_df, pre_trained = True, save_parquet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4qqPD2lHYGZ"
   },
   "outputs": [],
   "source": [
    "cleaned_df.unpersist()\n",
    "del cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxj6t2_dF4Xr"
   },
   "outputs": [],
   "source": [
    "train_df_word2Vec, test_df_word2Vec = data_split(features_df_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2EYqMbPGJ8C"
   },
   "outputs": [],
   "source": [
    "features_df_word2vec.unpersist()\n",
    "del features_df_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDEbF2dkHOov"
   },
   "outputs": [],
   "source": [
    "model_predictions = logistic_regression_classifier(train_df_word2Vec, test_df_word2vec, \"Word2Vec\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbxVfILOHPh7"
   },
   "outputs": [],
   "source": [
    "model_predictions = random_forest_classifier(train_df_word2Vec, test_df_word2vec, \"Word2Vec\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zViboEO-HQTq"
   },
   "outputs": [],
   "source": [
    "model_predictions = linear_svc_classifier(train_df_word2Vec, test_df_word2vec, \"Word2Vec\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsQL7ILqHRH7"
   },
   "outputs": [],
   "source": [
    "model_predictions = mlp_classifier(train_df_word2Vec, test_df_word2vec, \"Word2Vec\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aauh5czGHSNq"
   },
   "outputs": [],
   "source": [
    "train_df_word2Vec.unpersist()\n",
    "test_df_word2vec.unpersist()\n",
    "\n",
    "del train_df_word2Vec\n",
    "del test_df_word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u41HQ9U4r2rQ",
    "tags": []
   },
   "source": [
    "### Models + BERT (small_bert_L2_768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzyO0PF6CcUq"
   },
   "outputs": [],
   "source": [
    "# features_df_bert = spark.read.parquet(\"bert_features.parquet\")\n",
    "features_df_bert = bert_embedding(cleaned_df, save_parquet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yc8zEvGNr2rR"
   },
   "outputs": [],
   "source": [
    "cleaned_df.unpersist()\n",
    "del cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "or4-IcTUr2rR"
   },
   "outputs": [],
   "source": [
    "train_df_bert, test_df_bert = data_split(features_df_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPmcMaAFr2rR"
   },
   "outputs": [],
   "source": [
    "features_df_bert.unpersist()\n",
    "del features_df_bert\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSRZQt8-r2rS"
   },
   "outputs": [],
   "source": [
    "model_predictions = logistic_regression_classifier(train_df_bert, test_df_bert, \"BERT_small\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aK5tpNxjr2rS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = random_forest_classifier(train_df_bert, test_df_bert, \"BERT_small\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cl3z21TNr2rS"
   },
   "outputs": [],
   "source": [
    "model_predictions = linear_svc_classifier(train_df_bert, test_df_bert, \"BERT_small\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HHJ1iRVr2rS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = mlp_classifier(train_df_bert, test_df_bert, \"BERT_small\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmyYi6iDr2rS"
   },
   "outputs": [],
   "source": [
    "train_df_bert.unpersist()\n",
    "test_df_bert.unpersist()\n",
    "model_predictions.unpersist()\n",
    "\n",
    "del train_df_bert\n",
    "del test_df_bert\n",
    "del model_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muOJ-M7dr2rT",
    "tags": []
   },
   "source": [
    "### Models + BERT (bert_base_uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eD2WN-4r2rT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features_df_bert = spark.read.parquet(\"bert_features.parquet\")\n",
    "features_df_bert = bert_embedding(cleaned_df, save_parquet=False, model=\"bert_base_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfEmDyvNr2rT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_df.unpersist()\n",
    "del cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pIdbtU1r2rT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_bert, test_df_bert = data_split(features_df_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvxiamnQr2rU"
   },
   "outputs": [],
   "source": [
    "features_df_bert.unpersist()\n",
    "del features_df_bert\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqD1AYuyr2rU"
   },
   "outputs": [],
   "source": [
    "model_predictions = logistic_regression_classifier(train_df_bert, test_df_bert, \"BERT_uncased\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvdo03gMr2rU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = random_forest_classifier(train_df_bert, test_df_bert, \"BERT_uncased\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48T4F4zDr2rU"
   },
   "outputs": [],
   "source": [
    "model_predictions = linear_svc_classifier(train_df_bert, test_df_bert, \"BERT_uncased\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqSoh42ur2rU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = mlp_classifier(train_df_bert, test_df_bert, \"BERT_uncased\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9-paY_ur2rU"
   },
   "outputs": [],
   "source": [
    "train_df_bert.unpersist()\n",
    "test_df_bert.unpersist()\n",
    "model_predictions.unpersist()\n",
    "\n",
    "del train_df_bert\n",
    "del test_df_bert\n",
    "del model_predictions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_XjNG35q-_c"
   },
   "source": [
    "# **Sarcasm detection on Reddit**\n",
    "\n",
    "In this notebook, we addressed a binary classification problem aimed at detecting sarcasm in Reddit comments.\n",
    "\n",
    "We chose 3 feature engineering methods and 4 classification models and compared them to find out which combination performs better:\n",
    "- Feature engineering: TF-IDF, Word2Vec, and BERT.\n",
    "\n",
    "- Classification models: Logistic regression, Random forest, Linear SVC, and Multilayer perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MxG9URvZX6F"
   },
   "source": [
    "## **Spark Setup**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ug3pbXm7QZ5a"
   },
   "source": [
    "### 1. Install PySpark and related dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_m0ZZHdpAL1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# java\n",
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# !wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop2.7.tgz\n",
    "# !tar xf spark-3.2.4-bin-hadoop2.7.tgz\n",
    "# !rm -rf /content/spark-3.2.4-bin-hadoop2.7.tgz\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTWh4NkcrFqF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Krt1RPb1rK53",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setting java path as environment variable\n",
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GX32q-n1rLbb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "!pip install spark-nlp==4.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZT5w4kaYGcZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kw7DLnymt5lD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAJfhtqAgHep"
   },
   "source": [
    "### 2. Import useful Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNwfUBoUgNLP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.feature import VectorAssembler, SQLTransformer, Normalizer\n",
    "from pyspark.sql.functions import udf, col, lower, trim, regexp_replace, transform\n",
    "\n",
    "import sparknlp\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7SUuHFUgfhs"
   },
   "source": [
    "### 3. Create Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ak2WG_p3uaIi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Hj0MDQTghZ0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"1g\") \\\n",
    "    .config(\"spark.driver.memory\",\"32G\") \\\n",
    "    .config(\"spark.python.worker.memory\",\"32G\") \\\n",
    "    .config(\"spark.sql.analyzer.maxIterations\", \"6000\") \\\n",
    "    .config(\"spark.driver.cores\", \"10\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"32G\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.4.2\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "print(\"Apache Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xi0BzByXufxI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "type(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvgLAqZqvG5L"
   },
   "source": [
    "## **Dataset**\n",
    "\n",
    "The original dataset used in this notebook is available [here](https://www.kaggle.com/datasets/toygarr/datasets-for-natural-language-processing), in the *sarcasm* folder.\n",
    "This dataset contains 30k sarcastic comments from the Internet commentary website Reddit.\n",
    "The comments are pre-processed by making lower case and removing punctuations, hashtags, usernames and html tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSWbPHmjuCBS"
   },
   "source": [
    "### Download dataset from Kaggle\n",
    "\n",
    "In order to get the dataset from Kaggle library, a kaggle.json file is required. It can be downloaded from our personal account page and uploaded in the same folder of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FqfesOeZ2li",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Install the Kaggle library\n",
    "! pip install kaggle\n",
    "\n",
    "# 2. Make a directory named “.kaggle”\n",
    "! mkdir ~/.kaggle\n",
    "\n",
    "# 3. Copy the “kaggle.json” into this new directory\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "\n",
    "# 4. Allocate the required permission for this file.\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# 5. Download the dataset\n",
    "! kaggle datasets download toygarr/datasets-for-natural-language-processing\n",
    "\n",
    "# unzip the directory\n",
    "! unzip datasets-for-natural-language-processing.zip\n",
    "\n",
    "# delete zip file\n",
    "! rm -rf datasets-for-natural-language-processing.zip\n",
    "\n",
    "# Make a directory to save final models\n",
    "! mkdir models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfCVZgoavfcb"
   },
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-irriTP00UQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "\n",
    "    df1 = spark.read.csv(\"data/sarcasm/train.csv\", header=True)\n",
    "    df2 = spark.read.csv(\"data/sarcasm/test.csv\", header=True)\n",
    "\n",
    "    df = df1.union(df2)\n",
    "\n",
    "    #####################################\n",
    "\n",
    "    # rename columns\n",
    "    print(\"Dataframe size (n. of rows): {:d}\\n\".format(df.count()))\n",
    "    df = df.withColumnRenamed(\"Y\", \"label\")\n",
    "    df = df.withColumnRenamed(\"text\", \"comment\")\n",
    "    print(\"Dataframe schema:\")\n",
    "    df.printSchema()\n",
    "    df.show(10)\n",
    "\n",
    "    #####################################\n",
    "\n",
    "    print(\"Number of NULL comments: {:d}\".format(df.where(col(\"comment\").isNull()).count()))\n",
    "    print(\"Number of NULL labels: {:d}\\n\".format(df.where(col(\"label\").isNull()).count()))\n",
    "    # remove NULL entry/ies\n",
    "    df = df.na.drop(subset=[\"comment\"])\n",
    "    # remove NULL labels\n",
    "    df = df.na.drop(subset=[\"label\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PM9hJLdTeceE"
   },
   "source": [
    "### Data Pre-processing\n",
    "\n",
    "Starting from the raw text data, we apply a standard NLP preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kw3sn1DBkdbW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer, LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf, col, rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIr7_usce3z-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(df, column_name=\"comment\"):\n",
    "\n",
    "    # Text preprocessing pipeline\n",
    "    print(\"***** Text Preprocessing Pipeline *****\\n\")\n",
    "\n",
    "    # convert the text into a Spark-NLP annotator-ready form\n",
    "    documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol(column_name) \\\n",
    "     .setOutputCol('document')\n",
    "\n",
    "    tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['document']) \\\n",
    "     .setOutputCol('token')\n",
    "\n",
    "    normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setLowercase(True) \\\n",
    "    .setCleanupPatterns([\"\"\"[^\\w\\d\\s]\"\"\"]) # remove punctuations (keep alphanumeric chars)\n",
    "\n",
    "    # note that lemmatizer needs a dictionary. So we used the pre-trained model (note that it defaults to english)\n",
    "    lemmatizer = LemmatizerModel.pretrained(\"lemma_antbnc\") \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('lemma')\n",
    "\n",
    "    stopwords_cleaner = StopWordsCleaner.pretrained(\"stopwords_en\") \\\n",
    "     .setInputCols(['lemma']) \\\n",
    "     .setOutputCol('clean_lemma') \\\n",
    "     .setCaseSensitive(False)\n",
    "\n",
    "    # convert tokens back to human-readable form\n",
    "    finisher = Finisher() \\\n",
    "     .setInputCols(['clean_lemma']) \\\n",
    "     .setOutputCols(\"finisher_result\") \\\n",
    "     .setCleanAnnotations(False) \\\n",
    "     .setOutputAsArray(True)\n",
    "\n",
    "    pipeline = Pipeline() \\\n",
    "     .setStages([\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        normalizer,\n",
    "        lemmatizer,\n",
    "        stopwords_cleaner,\n",
    "        finisher\n",
    "     ])\n",
    "\n",
    "    cleaned_df = pipeline.fit(df).transform(df).select(['comment', 'document', 'clean_lemma', 'finisher_result', 'label'])\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    cleaned_df = cleaned_df.filter(\"finisher_result != array() and finisher_result != array('') and finisher_result != array(null)\")\n",
    "    cleaned_df = cleaned_df.dropDuplicates([\"finisher_result\",\"label\"])\n",
    "\n",
    "    # remove rows with same comments but different labels\n",
    "    cleaned_df_2 = cleaned_df.groupby(\"finisher_result\").agg(count(\"finisher_result\").alias(\"polletti\")).filter(col(\"polletti\")>1)\n",
    "    cleaned_df = cleaned_df.join(cleaned_df_2, [\"finisher_result\"], 'left_anti')\n",
    "\n",
    "    cleaned_df_2.unpersist()\n",
    "    del cleaned_df_2\n",
    "\n",
    "    print(\"Dataframe schema:\")\n",
    "    cleaned_df.printSchema()\n",
    "\n",
    "    cleaned_df.show(10)\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    print(\"Let's see our dataset's class distribution:\")\n",
    "    cleaned_df.groupBy('label').count().show()\n",
    "\n",
    "    # The dataset is slightly unbalanced, we apply a downsampling so that the two classes had the same number of samples.\n",
    "    df_0 = cleaned_df.filter(col(\"label\") == 0).orderBy(rand()).limit(cleaned_df.filter(col(\"label\") == 1).count())\n",
    "    df_1 = cleaned_df.filter(col(\"label\") == 1)\n",
    "\n",
    "    cleaned_df = df_0.union(df_1)\n",
    "\n",
    "    print(\"Let's check if our dataset is now balanced:\")\n",
    "    # Now we have 13_552 samples per class\n",
    "    cleaned_df.groupBy('label').count().show()\n",
    "\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5A1iplQHqZt"
   },
   "source": [
    "## **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uH9bbYFXJKx-"
   },
   "source": [
    "### **TF-IDF + PCA**\n",
    "\n",
    "*Term frequency-inverse document frequency* (TF-IDF) is a feature vectorization method widely used in text mining to reflect the importance of a term to a document in the corpus.\n",
    "The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.\n",
    "\n",
    "*Principal component analysis* (PCA) is a dimensionality reduction technique, which is a process of reducing the number of variables under consideration.\n",
    "More specifically, it is a statistical method to find a rotation such that the first coordinate has the largest variance possible, and each succeeding coordinate, in turn, has the largest variance possible. The columns of the rotation matrix are called principal components.\\\n",
    "We use it to project the vector of extracted features into a low-dimensional space, thus selecting only the most important ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeYKO5c-CNkN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, CountVectorizer, IDF\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRDEUhwAHpxh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tfidf_pca_features(cleaned_df, column_name=\"finisher_result\", save_parquet=False, load_pipeline=False):\n",
    "\n",
    "    if load_pipeline:\n",
    "        features_pca_fitted = PipelineModel.load(\"PCA_pipeline\")\n",
    "    else:\n",
    "\n",
    "        ### TF-IDF ###\n",
    "        hashingTF = HashingTF(inputCol=column_name, outputCol=\"tf_features\", numFeatures=10_000)\n",
    "        idf = IDF(inputCol=\"tf_features\", outputCol=\"idf_features\")\n",
    "\n",
    "        ### StandardScaler ###\n",
    "        scaler = StandardScaler(inputCol=\"idf_features\",\n",
    "                              outputCol=\"std_features\",\n",
    "                              withStd=True, withMean=True)\n",
    "\n",
    "        ### PCA ###\n",
    "        pca_model = PCA(k=500, inputCol=\"std_features\", outputCol=\"pca_features\")\n",
    "\n",
    "        tf_idf_pca_pipeline = Pipeline(stages=[hashingTF, idf, scaler, pca_model])\n",
    "        features_pca_fitted = tf_idf_pca_pipeline.fit(cleaned_df)\n",
    "\n",
    "        features_pca_fitted.write().overwrite().save(\"PCA_pipeline\")\n",
    "\n",
    "\n",
    "    features_pca = features_pca_fitted.transform(cleaned_df) \\\n",
    "                          .select(col(\"comment\"), col(\"pca_features\").alias(\"features\"), col(\"label\")) \\\n",
    "                          .withColumn(\"features\", vector_to_array(\"features\")) \\\n",
    "                          .select(['comment'] + ['label'] + [expr('features[' + str(x) + ']') for x in range(500)])\n",
    "\n",
    "    # cast each column to float\n",
    "    for col_name in features_pca.columns:\n",
    "        if col_name == \"comment\":\n",
    "            continue\n",
    "        features_pca = features_pca.withColumn(col_name, col(col_name).cast(FloatType()))\n",
    "\n",
    "    ### VectorAssembler ###\n",
    "    vector_assembler = VectorAssembler(inputCols=features_pca.columns[2:], outputCol='features').setHandleInvalid(\"skip\")\n",
    "    features_pca = vector_assembler.transform(features_pca).select('comment', 'features', 'label')\n",
    "\n",
    "    print(\"Dataframe schema:\")\n",
    "    features_pca.printSchema()\n",
    "    features_pca.show(10)\n",
    "\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    if save_parquet:\n",
    "        features_pca.write.mode(\"overwrite\").parquet(\"pca_features.parquet\")\n",
    "\n",
    "    return features_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZLyZPQHOplX"
   },
   "source": [
    "### **Word2Vec**\n",
    "\n",
    "Word2Vec is a technique that uses a (shallow) neural-network-based model to map each word to a vector of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Weede8WEP84e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import Word2VecModel, SentenceEmbeddings, Word2VecApproach\n",
    "from sparknlp import EmbeddingsFinisher\n",
    "from pyspark.ml.functions import vector_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSHJeqId00UT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def word2vec_embedding(cleaned_df, pre_trained = False, save_parquet=False):\n",
    "\n",
    "    df_word2vec = cleaned_df.select('document', 'clean_lemma', 'label', 'comment')\n",
    "\n",
    "    cleaned_df.unpersist()\n",
    "    del cleaned_df\n",
    "\n",
    "    if pre_trained:\n",
    "        embeddings = Word2VecModel.pretrained(\"word2vec_gigaword_wiki_300\", \"en\") \\\n",
    "            .setInputCols([\"clean_lemma\"]) \\\n",
    "            .setOutputCol(\"embeddings\")\n",
    "    else:\n",
    "        embeddings = Word2VecApproach() \\\n",
    "            .setInputCols([\"clean_lemma\"]) \\\n",
    "            .setOutputCol(\"embeddings\") \\\n",
    "            .setMaxIter(5) \\\n",
    "            .setVectorSize(300)\n",
    "\n",
    "    # the result of w2v are multiple vectors, we will collapse them into one using the average pooling strategy\n",
    "    embeddingsSentence = SentenceEmbeddings() \\\n",
    "      .setInputCols([\"document\", \"embeddings\"]) \\\n",
    "      .setOutputCol(\"sentence_embeddings\") \\\n",
    "      .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "    embeddingsFinisher = EmbeddingsFinisher() \\\n",
    "    .setInputCols(\"sentence_embeddings\") \\\n",
    "    .setOutputCols(\"embeddings_result\") \\\n",
    "    .setOutputAsVector(True) \\\n",
    "    .setCleanAnnotations(False)\n",
    "\n",
    "    word2vec_pipeline = Pipeline(stages=[embeddings, embeddingsSentence, embeddingsFinisher])\n",
    "\n",
    "    df_word2vec = word2vec_pipeline.fit(df_word2vec).transform(df_word2vec) \\\n",
    "                                 .select(col('comment'), col('label'), explode(\"embeddings_result\").alias('features')) \\\n",
    "                                 .withColumn(\"features\", vector_to_array(\"features\")) \\\n",
    "                                 .select(['comment'] + ['label'] + [expr('features[' + str(x) + ']') for x in range(300)])\n",
    "\n",
    "    # cast each column to float\n",
    "    for col_name in df_word2vec.columns:\n",
    "        if col_name == \"comment\":\n",
    "            continue\n",
    "        df_word2vec = df_word2vec.withColumn(col_name, col(col_name).cast(FloatType()))\n",
    "\n",
    "    ### VectorAssembler ###\n",
    "    vector_assembler = VectorAssembler(inputCols=df_word2vec.columns[2:], outputCol='features').setHandleInvalid(\"skip\")\n",
    "\n",
    "    features_assembled = vector_assembler.transform(df_word2vec).select('comment', 'features', 'label')\n",
    "\n",
    "    df_word2vec.unpersist()\n",
    "    del df_word2vec\n",
    "\n",
    "    print(\"Dataframe schema:\")\n",
    "    features_assembled.printSchema()\n",
    "    features_assembled.show(10)\n",
    "\n",
    "    if save_parquet:\n",
    "        features_assembled.write.mode(\"overwrite\").parquet(\"word2vec_features.parquet\")\n",
    "\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    return features_assembled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgeXosep6Ibf"
   },
   "source": [
    "### **BERT**\n",
    "\n",
    "*Bidirectional Encoder Representations from Transformers* (BERT) is a state-of-the-art transformer-based model for natural language processing (NLP). It is designed to capture contextual information from text by using a deep bidirectional architecture.\n",
    "BERT generates contextualized word embeddings, known as BERT embeddings, which are pre-trained on large amounts of unlabeled text and then fine-tuned for specific NLP tasks.\n",
    "\n",
    "The BERT models used in this notebook provides a **word-level embedding** using the BERT architecture. They take as input a sequence of tokens and generate contextualized embeddings for each token in the sequence. The models capture the context of each token based on its surrounding tokens in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNQqJBGEqovN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import BertEmbeddings, SentenceEmbeddings\n",
    "from sparknlp import EmbeddingsFinisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZZURiidCfwS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bert_embedding(cleaned_df, save_parquet=False, model=\"small_bert_L2_768\"):\n",
    "    df_bert = cleaned_df.select('document', 'clean_lemma', 'label', 'comment')\n",
    "\n",
    "    cleaned_df.unpersist()\n",
    "    del cleaned_df\n",
    "\n",
    "    # bert_base_uncased -> the model is imported from https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\n",
    "    # small_bert_L2_768 -> the model is imported from https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1\n",
    "    embeddings = BertEmbeddings.pretrained(model, 'en') \\\n",
    "                .setInputCols(\"document\", \"clean_lemma\") \\\n",
    "                .setOutputCol(\"embeddings\")\n",
    "    \n",
    "     # the result of BERT are multiple vectors, we will collapse them into one using the average pooling strategy\n",
    "    embeddingsSentence = SentenceEmbeddings() \\\n",
    "      .setInputCols([\"document\", \"embeddings\"]) \\\n",
    "      .setOutputCol(\"sentence_embeddings\") \\\n",
    "      .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "    finisher = EmbeddingsFinisher() \\\n",
    "              .setInputCols(\"sentence_embeddings\") \\\n",
    "              .setOutputCols(\"embeddings_result\")\n",
    "\n",
    "    bert_pipeline = Pipeline(stages=[embeddings, embeddingsSentence, finisher])\n",
    "\n",
    "    df_bert = bert_pipeline.fit(df_bert).transform(df_bert) \\\n",
    "                         .select(col('comment'), col('label').cast(FloatType()), explode(\"embeddings_result\").alias('features')) \\\n",
    "                         .select(['comment'] + ['label'] + [expr('features[' + str(x) + ']') for x in range(768)])\n",
    "\n",
    "    ### VectorAssembler ###\n",
    "    vector_assembler = VectorAssembler(inputCols=df_bert.columns[2:], outputCol='features').setHandleInvalid(\"skip\")\n",
    "\n",
    "    features_assembled = vector_assembler.transform(df_bert).select('comment', 'features', 'label')\n",
    "\n",
    "    df_bert.unpersist()\n",
    "    del df_bert\n",
    "\n",
    "    print(\"Dataframe schema:\")\n",
    "    features_assembled.printSchema()\n",
    "\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    if save_parquet:\n",
    "        features_assembled.write.mode(\"overwrite\").parquet(\"bert_features_768.parquet\")\n",
    "\n",
    "    return features_assembled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Split dataset\n",
    "Split original dataset into 2 subdatasets: training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_split(df_features):\n",
    "    print(\"Dataframe size in data_split: {:d} instances\".format(df_features.count()))\n",
    "\n",
    "    # Randomly split our original dataset into 80÷20 for training and test, respectively\n",
    "    train_df, test_df = df_features.randomSplit([0.8, 0.2], seed = 42)\n",
    "\n",
    "    df_features.unpersist()\n",
    "    del df_features\n",
    "\n",
    "    print(\"Training set size: {:d} instances\".format(train_df.count()))\n",
    "    print(\"Test set size: {:d} instances\".format(test_df.count()))\n",
    "\n",
    "    print(\"\\nLet's verify our datasets are still balanced:\")\n",
    "    train_df.groupBy('label').count().show()\n",
    "    test_df.groupBy('label').count().show()\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF5vgp3FwhWZ"
   },
   "source": [
    "## **Classification models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZSKaQYZwnyB"
   },
   "source": [
    "### **Logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvELNZT6xNnK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUb40P1j9U_i",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logistic_regression_classifier(train_df, test_df, suffix_path=\"\", save_model=False):\n",
    "\n",
    "    lr = LogisticRegression(featuresCol='features',\n",
    "                          labelCol='label',\n",
    "                          maxIter=100,\n",
    "                          regParam=0.3,\n",
    "                          elasticNetParam=0.8)\n",
    "\n",
    "    ### Search for the best model's parameters. ###\n",
    "\n",
    "    # We use a ParamGridBuilder to construct a grid of parameters to search over\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.0, 0.05, 0.1]) \\\n",
    "                .addGrid(lr.elasticNetParam, [0.5, 0.8, 1.0]) \\\n",
    "                .build()\n",
    "\n",
    "    cross_val = CrossValidator(estimator=lr,\n",
    "                             estimatorParamMaps=param_grid,\n",
    "                             evaluator=BinaryClassificationEvaluator(metricName=\"areaUnderROC\"),\n",
    "                             numFolds=5,\n",
    "                             parallelism=5)\n",
    "\n",
    "    # Run cross-validation, and choose the best set of parameters.\n",
    "    cv_model = cross_val.fit(train_df)\n",
    "\n",
    "    best_lr = cv_model.bestModel\n",
    "\n",
    "    print(\"--- Best model's parameters: ---\\n- reParam = {:f}\\n- elasticNetParam = {:f}\".format(best_lr.getRegParam(), best_lr.getElasticNetParam()))\n",
    "\n",
    "    lr_predictions = best_lr.transform(test_df)\n",
    "    lr_prediction.show(10)\n",
    "\n",
    "    if save_model:\n",
    "        path = \"models/logisticRegression_\" + suffix_path\n",
    "        best_lr.write().overwrite().save(path)\n",
    "\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    return lr_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcQEUTaFmYBV"
   },
   "source": [
    "### **Random forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDWYCPMsmhHj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dIqrNFXiiJR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_forest_classifier(train_df, test_df, suffix_path='',save_model=False):\n",
    "\n",
    "    rf = RandomForestClassifier(featuresCol='features',\n",
    "                              labelCol='label')\n",
    "\n",
    "    ### Search for the best model's parameters. ###\n",
    "\n",
    "    # We use a ParamGridBuilder to construct a grid of parameters to search over\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [3, 5, 8]) \\\n",
    "    .addGrid(rf.numTrees, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "    cross_val = CrossValidator(estimator=rf,\n",
    "                             estimatorParamMaps=param_grid,\n",
    "                             evaluator=BinaryClassificationEvaluator(metricName=\"areaUnderROC\"),\n",
    "                             numFolds=5,\n",
    "                             parallelism=5)\n",
    "\n",
    "    # Run cross-validation, and choose the best set of parameters.\n",
    "    cv_model = cross_val.fit(train_df)\n",
    "\n",
    "    best_rf = cv_model.bestModel\n",
    "\n",
    "    print(\"--- Best model's parameters: ---\\n- maxDepth = {:d}\\n- numTrees = {:d}\".format(best_rf.getMaxDepth(), best_rf.getNumTrees()))\n",
    "\n",
    "    rf_predictions = best_rf.transform(test_df)\n",
    "    rf_predictions.show(10)\n",
    "\n",
    "    if save_model:\n",
    "        path = \"models/randomForest_\" + suffix_path\n",
    "        best_rf.write().overwrite().save(path)\n",
    "\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    return rf_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyj7FtWimhir",
    "tags": []
   },
   "source": [
    "### **Linear Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjmtSKKXmlw_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ic7Yzu_BdsH7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_svc_classifier(train_df, test_df, suffix_path='', save_model=False):\n",
    "\n",
    "    lsvc = LinearSVC(featuresCol='features', labelCol='label', regParam=0.1)\n",
    "\n",
    "    ### Search for the best model's parameters. ###\n",
    "\n",
    "    # We use a ParamGridBuilder to construct a grid of parameters to search over\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "                .addGrid(lsvc.regParam, [0.0, 0.05, 0.1]) \\\n",
    "                .addGrid(lsvc.maxIter, [50, 100]) \\\n",
    "                .build()\n",
    "\n",
    "    cross_val = CrossValidator(estimator=lsvc,\n",
    "                             estimatorParamMaps=param_grid,\n",
    "                             evaluator=BinaryClassificationEvaluator(metricName=\"areaUnderROC\"),\n",
    "                             numFolds=5,\n",
    "                             parallelism=5)\n",
    "\n",
    "    # Run cross-validation, and choose the best set of parameters.\n",
    "    cv_model = cross_val.fit(train_df)\n",
    "\n",
    "    best_lsvc = cv_model.bestModel\n",
    "\n",
    "    print(\"--- Best model's parameters: ---\\n- regParam = {:f}\\n- maxIter = {:d}\".format(best_lsvc.getRegParam(), best_lsvc.getMaxIter()))\n",
    "\n",
    "    lsvc_predictions = best_lsvc.transform(test_df)\n",
    "    lsvc_predictions.show(10)\n",
    "\n",
    "    if save_model:\n",
    "        path = \"models/linearSVC_\" + suffix_path\n",
    "        best_lsvc.write().overwrite().save(path)\n",
    "\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    return lsvc_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uf8ZO9nTmlPQ"
   },
   "source": [
    "### **Multilayer perceptron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaNVvY-8mqqp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-qlmPpDLa0H",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mlp_classifier(train_df, test_df, suffix_path='', save_model=False):\n",
    "\n",
    "    if suffix_path == 'Word2Vec':\n",
    "        num_features = 300\n",
    "    elif suffix_path == 'tfidf':\n",
    "        num_features = 500\n",
    "    elif suffix_path == 'BERT_uncased' or suffix_path == 'BERT_small':\n",
    "        num_features = 768\n",
    "    else:\n",
    "        num_features = 1\n",
    "\n",
    "    # specify layers for the neural network:\n",
    "    input_layer = num_features # features\n",
    "    output_layer = 2 # number of classes\n",
    "    layers = [input_layer, 64, 32, output_layer]\n",
    "\n",
    "    mlp = MultilayerPerceptronClassifier(featuresCol='features', labelCol='label',\n",
    "                                       maxIter=100, layers=layers, blockSize=256, seed=42)\n",
    "\n",
    "    mlp_model = mlp.fit(train_df)\n",
    "\n",
    "    print(\"--- Model's parameters: ---\\n- layers = {:s}\".format(str(mlp_model.getLayers())))\n",
    "\n",
    "    mlp_predictions = mlp_model.transform(test_df)\n",
    "    mlp_predictions.show(10)\n",
    "\n",
    "    if save_model:\n",
    "        path = \"models/mlp_\" + suffix_path\n",
    "        mlp_model.write().overwrite().save(path)\n",
    "\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    return mlp_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CN2C01Gx7Y14"
   },
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "178qa1FbMvcg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuDinRMUp_QK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluation(model_predictions):\n",
    "    print(\"***** Test Set *****\")\n",
    "\n",
    "    preds_df = model_predictions.select(\"comment\", \"label\", \"prediction\").toPandas()\n",
    "\n",
    "    print(\"\\nShow some examples of miss-predictions: \")\n",
    "    display(preds_df[preds_df['prediction'] != preds_df['label']].head(100))\n",
    "\n",
    "    print(\"\\nClassification report:\")\n",
    "    clf_report = classification_report(y_true=preds_df['label'], y_pred=preds_df['prediction'], zero_division=0)\n",
    "    print(clf_report)\n",
    "\n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "    evaluator.setRawPredictionCol('rawPrediction')\n",
    "\n",
    "    # calculate AUROC\n",
    "    print('Area Under ROC Curve (AUROC): %.3f' % evaluator.evaluate(model_predictions, {evaluator.metricName: \"areaUnderROC\"}))\n",
    "\n",
    "    # calculate AUPR\n",
    "    print('Area Under Precision-Recall Curve (AUPR): %.3f' % evaluator.evaluate(model_predictions, {evaluator.metricName: \"areaUnderPR\"}))\n",
    "\n",
    "    # calculate MCC\n",
    "    print(\"Matthews Correlation Coefficient (MCC): \", matthews_corrcoef(preds_df['label'], preds_df['prediction']))\n",
    "\n",
    "    print(\"\\n***** Test Set *****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noKx6G_Gp-5S"
   },
   "source": [
    "## **Experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qTKZHEWBvGn"
   },
   "source": [
    "### **Data loading and pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3DxkBnx_KQj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-HIQPRt_Ne8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_df = clean_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXv09wBm_fDU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.unpersist()\n",
    "del df\n",
    "\n",
    "print(\"Garbage collector: collected %d objects\" % (gc.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4CvxuNv00Uh",
    "tags": []
   },
   "source": [
    "### **Models + TF-IDF + PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYLfcRITB236",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features_df_tfidf = spark.read.parquet(\"tfidf_features.parquet\")\n",
    "features_df_tfidf = tfidf_pca_features(cleaned_df, save_parquet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Trst28100Ui",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_df.unpersist()\n",
    "del cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKBhUweLsFME",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_tfidf, test_df_tfidf = data_split(features_df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYOfXN3JsGhk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_df_tfidf.unpersist()\n",
    "del features_df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6V-p1Vv00Ui",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = logistic_regression_classifier(train_df_tfidf, test_df_tfidf, \"tfidf\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYSPFJFD00Ui",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = random_forest_classifier(train_df_tfidf, test_df_tfidf, \"tfidf\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQMyc6Jl00Uj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = linear_svc_classifier(train_df_tfidf, test_df_tfidf, \"tfidf\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9rJglo_00Uj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = mlp_classifier(train_df_tfidf, test_df_tfidf, \"tfidf\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKvqIsho00Uk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_tfidf.unpersist()\n",
    "test_df_tfidf.unpersist()\n",
    "\n",
    "del train_df_tfidf\n",
    "del test_df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGinYb4800Uf",
    "tags": []
   },
   "source": [
    "### **Models + Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2wq3b9O_h5U",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features_df_word2vec = spark.read.parquet(\"word2vec_features.parquet\")\n",
    "word2vec_features = word2vec_embedding(cleaned_df, save_parquet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGSy7keb00Ug",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_df.unpersist()\n",
    "del cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSzYsg1ErHDV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_word2Vec, test_df_word2vec = data_split(word2vec_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNPc8gkwrS_-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "word2vec_features.unpersist()\n",
    "del word2vec_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cb5S9viHFhly",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = logistic_regression_classifier(train_df_word2Vec, test_df_word2vec, \"Word2Vec\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOhAs55Tkdcm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = random_forest_classifier(train_df_word2Vec, test_df_word2vec, \"Word2Vec\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2A6lv1zFVsfq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = linear_svc_classifier(train_df_word2Vec, test_df_word2vec, \"Word2Vec\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7bLErX23n1s",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = mlp_classifier(train_df_word2Vec, test_df_word2vec, \"Word2Vec\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTklKnrM00Uh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_word2Vec.unpersist()\n",
    "test_df_word2vec.unpersist()\n",
    "\n",
    "del train_df_word2Vec\n",
    "del test_df_word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egaT68qrGuWr"
   },
   "source": [
    "### **Models + Word2Vec (pre-trained)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8k7N5dAGwqj"
   },
   "outputs": [],
   "source": [
    "features_df_word2vec = word2vec_embedding(cleaned_df, pre_trained = True, save_parquet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-Pb8vOYGyzZ"
   },
   "outputs": [],
   "source": [
    "cleaned_df.unpersist()\n",
    "del cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rt0hZmi9G1hb"
   },
   "outputs": [],
   "source": [
    "train_df_word2Vec, test_df_word2vec = data_split(word2vec_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcDQgLteG4rK"
   },
   "outputs": [],
   "source": [
    "word2vec_features.unpersist()\n",
    "del word2vec_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9W4zpTFG6Sb"
   },
   "outputs": [],
   "source": [
    "model_predictions = logistic_regression_classifier(train_df_word2Vec, test_df_word2vec, \"Word2Vec\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sh8OzCNLG69Z"
   },
   "outputs": [],
   "source": [
    "model_predictions = random_forest_classifier(train_df_word2Vec, test_df_word2vec, \"Word2Vec\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydc9JUEgG8_y"
   },
   "outputs": [],
   "source": [
    "model_predictions = linear_svc_classifier(train_df_word2Vec, test_df_word2vec, \"Word2Vec\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSx4cQIfG-Ai"
   },
   "outputs": [],
   "source": [
    "model_predictions = mlp_classifier(train_df_word2Vec, test_df_word2vec, \"Word2Vec\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QUKz2niG_wJ"
   },
   "outputs": [],
   "source": [
    "train_df_word2Vec.unpersist()\n",
    "test_df_word2vec.unpersist()\n",
    "\n",
    "del train_df_word2Vec\n",
    "del test_df_word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XPzVRUV00Uk",
    "tags": []
   },
   "source": [
    "### **Models + BERT (small_bert_L2_768)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzyO0PF6CcUq"
   },
   "outputs": [],
   "source": [
    "features_df_bert = bert_embedding(cleaned_df, save_parquet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jYea2DY00Ul"
   },
   "outputs": [],
   "source": [
    "cleaned_df.unpersist()\n",
    "del cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDvOFtTksOnV"
   },
   "outputs": [],
   "source": [
    "train_df_bert, test_df_bert = data_split(features_df_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYtULsI_sQC-"
   },
   "outputs": [],
   "source": [
    "features_df_bert.unpersist()\n",
    "del features_df_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yz-FwOKO00Ul"
   },
   "outputs": [],
   "source": [
    "model_predictions = logistic_regression_classifier(train_df_bert, test_df_bert, \"BERT_small\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wudxcQlS00Um",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = random_forest_classifier(train_df_bert, test_df_bert, \"BERT_small\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wu2zbQDZ00Um"
   },
   "outputs": [],
   "source": [
    "model_predictions = linear_svc_classifier(train_df_bert, test_df_bert, \"BERT_small\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HjtLbvf00Um",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = mlp_classifier(train_df_bert, test_df_bert, \"BERT_small\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNuFtjX600Um"
   },
   "outputs": [],
   "source": [
    "train_df_bert.unpersist()\n",
    "test_df_bert.unpersist()\n",
    "model_predictions.unpersist()\n",
    "\n",
    "del train_df_bert\n",
    "del test_df_bert\n",
    "del model_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXjJBYZX00Um",
    "tags": []
   },
   "source": [
    "### **Models + BERT (bert_base_uncased)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UOmAguf00Un"
   },
   "outputs": [],
   "source": [
    "# features_df_bert = spark.read.parquet(\"bert_features.parquet\")\n",
    "features_df_bert = bert_embedding(cleaned_df, save_parquet=False, model=\"bert_base_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8wQbY5x00Un"
   },
   "outputs": [],
   "source": [
    "cleaned_df.unpersist()\n",
    "del cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lxz9QwiFsYhW"
   },
   "outputs": [],
   "source": [
    "train_df_bert, test_df_bert = data_split(features_df_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VdIDf9NsZoG"
   },
   "outputs": [],
   "source": [
    "features_df_bert.unpersist()\n",
    "del features_df_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wnr_PDC600Un"
   },
   "outputs": [],
   "source": [
    "model_predictions = logistic_regression_classifier(train_df_bert, test_df_bert, \"BERT_uncased\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjhB0n-T00Uo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = random_forest_classifier(train_df_bert, test_df_bert, \"BERT_uncased\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWARYE5b00Uo"
   },
   "outputs": [],
   "source": [
    "model_predictions = linear_svc_classifier(train_df_bert, test_df_bert, \"BERT_uncased\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2y9JLzO00Uo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = mlp_classifier(train_df_bert, test_df_bert, \"BERT_uncased\")\n",
    "evaluation(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kRs4sXq00Uo"
   },
   "outputs": [],
   "source": [
    "train_df_bert.unpersist()\n",
    "test_df_bert.unpersist()\n",
    "\n",
    "del train_df_bert\n",
    "del test_df_bert"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Ug3pbXm7QZ5a"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
